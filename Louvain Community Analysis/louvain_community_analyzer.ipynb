{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133f2b7b",
   "metadata": {},
   "source": [
    "## Louvain Community Analysis Generator\n",
    "This notebook accepts arbitrary columnar data in CSV format, creates an undirected graph from the columns of the data, and uses a Louvain community analysis algorithm to find \"communities\" within the graph.  From there, it identifies any communities that have a size that is statistically smaller than the other communities detected.  This is useful for identifying rare/abnormal tuples of data in your original table. \n",
    "\n",
    "Source: https://python-louvain.readthedocs.io/en/latest/api.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Louvain/Community modules.  Restart kernel after install.\n",
    "!pip install --upgrade community\n",
    "!pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "import collections\n",
    "from collections import Counter\n",
    "from networkx.drawing.nx_agraph import write_dot\n",
    "from networkx.algorithms import bipartite\n",
    "import networkx.algorithms.community as nxcom\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import community\n",
    "import os\n",
    "from community import community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd9ae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Supply a CSV file.  This can consist of any data you like; you can save a lot of processing time if you pre-filter\n",
    "# this to include only the columns you want to analyze.  The data should have a header row.\n",
    "# The file should be in the same directory as this notebook; it can be uploaded via the Files page in Jupyter Notebook.\n",
    "\n",
    "path = os.getcwd()\n",
    "file = input(\"Enter the location of a CSV file:\")\n",
    "\n",
    "try:\n",
    "    sheetpath = os.path.join(path, file)\n",
    "    df = pd.read_csv(sheetpath, header=0)\n",
    "    print('Ingested ' + str(df.shape[0]) + ' lines of data')\n",
    "except Exception:\n",
    "    print('File not found, please try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an undirected NetworkX graph out of the parent-child process data.\n",
    "# We will then attempt to identify communities within the graph results.\n",
    "network = nx.Graph()  # undirected graph\n",
    "\n",
    "# Build graph nodes and weighted edges\n",
    "for i in range(len(df)):\n",
    "    entry = df.iloc[i]\n",
    "    for j in range(len(entry) - 1):  # this is the number of columns\n",
    "        selector = df.columns[j]\n",
    "        value1 = df.iloc[i][j]\n",
    "        value2 = df.iloc[i][j + 1]\n",
    "\n",
    "        if not value1 in network:\n",
    "            if j == 0:  # first column, so this is the originator field\n",
    "                network.add_node(value1, type=selector, color=\"blue\", size=2, weight=1)\n",
    "            else:  # it's a downstream node\n",
    "                network.add_node(value1, type=selector, size=1, weight=1)\n",
    "        if not network.has_edge(value1, value2):\n",
    "            network.add_edge(value1, value2, weight=1, arrows=True, penwidth=1)\n",
    "        else:\n",
    "            network[value1][value2][\"weight\"] += 1\n",
    "            network.add_edge(\n",
    "                value1,\n",
    "                value2,\n",
    "                weight=network[value1][value2][\"weight\"],\n",
    "                arrows=True,\n",
    "                penwidth=network[value1][value2][\"weight\"],\n",
    "            )\n",
    "        j = j + 1\n",
    "    i = i + 1\n",
    "    \n",
    "# Render the graph\n",
    "nx.draw_spring(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Louvain community detection algorithm, to identify unique communities within the graph.\n",
    "# Use Matplotlib to display the resultant graph, with each community in a different color.\n",
    "\n",
    "partition = community_louvain.best_partition(network)\n",
    "\n",
    "# generate the graph\n",
    "pos = nx.spring_layout(network)\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap(\"viridis\", max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(\n",
    "    network,\n",
    "    pos,\n",
    "    partition.keys(),\n",
    "    node_size=40,\n",
    "    cmap=cmap,\n",
    "    node_color=list(partition.values()),\n",
    ")\n",
    "nx.draw_networkx_edges(network, pos, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d16004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe from the \"partition\" dict, that will contain each process \n",
    "# in the original data and what community it belongs to\n",
    "histodf = pd.DataFrame.from_dict(partition, orient=\"index\", columns=[\"communitylabel\"])\n",
    "\n",
    "# Group this new dataframe by the Count of items per community label.  \n",
    "# This will show us the size of each community.\n",
    "newgrouped = histodf.groupby([\"communitylabel\"]).agg({\"communitylabel\": [\"count\"]})\n",
    "\n",
    "print(newgrouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now need to identify any communities that are statisically smaller than the others, \n",
    "# indicating that they are the most rare.\n",
    "# To do this: Create a boxplot of the counts in each community, and identify as \"anomalous\" \n",
    "# if the community size is below the first quartile of the boxplot. \n",
    "fig2 = px.box(newgrouped[\"communitylabel\"][\"count\"], points=\"outliers\")\n",
    "fig2.update_traces(quartilemethod=\"exclusive\")\n",
    "\n",
    "threshold = newgrouped[\"communitylabel\"][\"count\"].describe()[\"25%\"]  # this is the first quartile\n",
    "\n",
    "# The Inter-Quartile Range (IQR) is defined as the value of the 75th quartile - value of 25th quartile.\n",
    "# If you would like to use the lower fence of the boxplot as the threshold instead of the first quartile\n",
    "# (which will reduce false positives at a cost of potentially increasing false negatives), \n",
    "# uncomment the following line:\n",
    "#threshold = newgrouped['communitylabel']['count'].describe()[\"25%\"] - 1.5 * (newgrouped['communitylabel']['count'].describe()[\"75%\"] - newgrouped['communitylabel']['count'].describe()[\"25%\"])\n",
    "\n",
    "# Create a dataframe of any community labels identified as statistically small (and therefore anomalous)\n",
    "anomalousvalues = newgrouped[newgrouped[\"communitylabel\"][\"count\"] <= threshold]\n",
    "\n",
    "print(anomalousvalues)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b07d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any outliers found, return the defined community from the original dataset\n",
    "rarecommunities = pd.DataFrame()  # empty df\n",
    "\n",
    "for communitylabel in anomalousvalues.index:\n",
    "    rarecommunities = rarecommunities.append(\n",
    "        histodf[histodf[\"communitylabel\"] == communitylabel]\n",
    "    )\n",
    "\n",
    "print(rarecommunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Output dataframe, which consists of any identified anomalies from the original data \n",
    "\n",
    "if rarecommunities.shape[0] > 0:\n",
    "    output = pd.DataFrame()\n",
    "    for communitylabel in rarecommunities[\"communitylabel\"]:\n",
    "        for column in df.columns:\n",
    "            output = output.append(df[(df[column] == \n",
    "                                   (rarecommunities.loc[rarecommunities[\"communitylabel\"] == communitylabel].index[0]))])\n",
    "\n",
    "    # Clean the data up a bit\n",
    "    output.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Identified \" + str(output.shape[0]) + \" anomalously small communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original data from any small communities identified\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd11f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump any results to CSV for further investigation\n",
    "output.to_csv(\"rarecommunities.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
