{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8117d0",
   "metadata": {},
   "source": [
    " # Finding Periodic Process Connectivity With Poisson Logic\n",
    " \n",
    " This notebook builds on logic published by Elastic to find periodic/beaconing\n",
    " processes.  TL;DR: independent events over time should follow a Poisson distribution.  Events that\n",
    " are more regular/metronomic are no longer independent, and are no longer Poisson-distributed. We can\n",
    " use this to identify processes that open network connectivity on a more metronomic basis, and therefore\n",
    " look more like beacons than benign connectivity.\n",
    " See https://www.elastic.co/blog/identifying-beaconing-malware-using-elastic for the full writeup.\n",
    "\n",
    " For this routine, needed to convert Elastic's logic to Python, and collect the right data. \n",
    " In lieu of Netflow data, instead used Sysmon EID3: Network Connection.  This identifies the machine and\n",
    " process that opened the connection, but not bytes-in or bytes-out.  So, this logic will identify periodicity\n",
    " by only process and timestamp. \n",
    " \n",
    "Below are the Elastic-like searches to generate data for this notebook.  Search is derived from Elastic logic at https://github.com/elastic/detection-rules/releases/tag/ML-Beaconing-20211216-1.  It had to be split into 2 searches, since the regex was too long.  Concatenate the results into a single CSV.  You may also need to restrict the SourceIP, if size of output\n",
    "is a concern.\n",
    "\n",
    "event_id:3 (NOT (DestinationIp:10.0.0.0/8)) (NOT (Image:/.*(AddressBookSourceSync|Adobe_CCX|Adobe |AdobeCollab|accountsd|akd|apsd|atmgr|assistantd|backgroundTaskHost|BackgroundTransferHost|Brave Browser Helper|CalendarAgent|CCXProcess|chrome|Code Helper|CompatTelRunner|commerce|Core Sync|default-browser-agent|DeliveryService|DeviceCensus|Docker|Dropbox|Dsapi|elastic-|esensor|EXCEL|explorer|filebeat|FileCoAuth|firefox|GitHub Desktop Helper|Google Chrome Helper|google_guest_agent|GCEWindowsAgent|Google Drive|GoogleUpdate|IMRemoteURLConnectionAgent|keybase|ksfetch).*/)) TABLE @timestamp,SourceHostname,User,Image,SourceIp  \n",
    "\n",
    "AND \n",
    "\n",
    "event_id:3 (NOT (DestinationIp:10.0.0.0/8)) (NOT (Image:/.*(mcautoreg|metricbeat|mdmclient|Mail|MMSSHOST|Microsoft Excel|Microsoft OneNote|Microsoft PowerPoint|Microsoft Teams|Microsoft Update|Microsoft Word|ModuleCoreService|msedge|node|nsurlsessiond|OfficeC2RClient|ONENOTE|officesvcmgr|OfficeClickToRun|OneDrive|parsec|pingsender|SDXHelper|SearchApp|ServiceLayer|Skype for Business|Slack|smartscreen|softwareupdated|Spotify|ssm-|syspolicyd|SystemIdleCheck|taskhostw|Teams|trustd|updater|WINWORD|WhatsApp Helper|xpcproxy|Zoom).*/)) TABLE @timestamp,SourceHostname,User,Image,SourceIp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things.\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import poisson\n",
    "pd.options.display.html.use_mathjax = False\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a5a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest the data you just generated. \n",
    "file = input(\"Enter the location of a CSV file:\")\n",
    "df = pd.read_csv(file, encoding=\"ANSI\", header=0, parse_dates=[\"@timestamp\"])\n",
    "df = df.dropna(axis=0)\n",
    "df.columns = ['timestamp', 'computername', 'user', 'process', 'sourceIP']\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.index = df['timestamp']\n",
    "print('Ingested ' + str(df.shape[0]) + ' lines of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "# Set some defaults.  Tweak em later if needed.\n",
    "\n",
    "# Experimental data shows that for a non-Poisson process (in other words, something more \"beacon-y\"), when the sampling\n",
    "# period equals the beacon interval, the RV approaches 0.08 and the CV approaches 0.2.  Search for this, over various \n",
    "# sampling periods.  Adjust targets as needed.\n",
    "cvtarget = 0.2\n",
    "cvrange = 0.65 * cvtarget\n",
    "rvtarget = 0.08\n",
    "rvrange = 0.5 * rvtarget\n",
    "samplingperiods = ['30s', '60s', '120s', '300s', '600s', '1200s', '1800s', '3600s', '14400s'] \n",
    "minimum_executions = 20\n",
    "outfile = 'c:\\\\hunting\\\\poisson\\\\identifiedbeacons.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66001e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove selected Images (executables), based on Elastic's config at https://github.com/elastic/detection-rules/releases/tag/ML-Beaconing-20211216-1\n",
    "stufftodrop = \"(AddressBookSourceSync|Adobe_CCX|Adobe |AdobeCollab|accountsd|akd|apsd|atmgr|assistantd|backgroundTaskHost|\"\n",
    "stufftodrop += \"BackgroundTransferHost|Brave Browser Helper|CalendarAgent|CCXProcess|chrome|Code Helper|CompatTelRunner|\"\n",
    "stufftodrop += \"commerce|Core Sync|default-browser-agent|DeliveryService|DeviceCensus|Docker|Dropbox|Dsapi|elastic-|esensor|\"\n",
    "stufftodrop += \"EXCEL|explorer|filebeat|FileCoAuth|firefox|GitHub Desktop Helper|Google Chrome Helper|google_guest_agent|\"\n",
    "stufftodrop += \"GCEWindowsAgent|Google Drive|GoogleUpdate|IMRemoteURLConnectionAgent|keybase|ksfetch|mcautoreg|metricbeat|\"\n",
    "stufftodrop += \"mdmclient|Mail|MMSSHOST|Microsoft Excel|Microsoft OneNote|Microsoft PowerPoint|Microsoft Teams|\"\n",
    "stufftodrop += \"Microsoft Update|Microsoft Word|ModuleCoreService|msedge|node|nsurlsessiond|OfficeC2RClient|ONENOTE|\"\n",
    "stufftodrop += \"officesvcmgr|OfficeClickToRun|OneDrive|parsec|pingsender|SDXHelper|SearchApp|ServiceLayer|Skype for Business|\"\n",
    "stufftodrop += \"Slack|smartscreen|softwareupdated|Spotify|ssm-|syspolicyd|SystemIdleCheck|taskhostw|Teams|trustd|\"\n",
    "stufftodrop += \"updater|WebSense Endpoint|WINWORD|WhatsApp Helper|xpcproxy|Zoom)\"\n",
    "rowstoremove = df['process'].str.contains(stufftodrop)\n",
    "df = df[~rowstoremove]\n",
    "print('Reduced to ' + str(df.shape[0]) + ' lines of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe0a9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each sampling period: resample the data, and find the count, stdev, mean, variance, coefficient of variance, and \n",
    "# relative variance of each process execution, per computer, per process.  This will create a Multi-index dataframe.\n",
    "\n",
    "# This logic uses 2 consecutive groupby operations, to result in a multi-index dataframe by computername, and by process.\n",
    "# It then performs aggregate statistics on the multiindexed data.\n",
    "\n",
    "# Delete the output file, if it exists\n",
    "if os.path.exists(outfile):\n",
    "    os.remove(outfile)\n",
    "    \n",
    "# Start with an empty dataframe\n",
    "found = pd.DataFrame()    \n",
    "    \n",
    "for period in tqdm(samplingperiods):\n",
    "       \n",
    "    # Create a multi-indexed table from our data, by computername, then by process, then by timestamp\n",
    "    grouped = df.groupby(['computername', 'process']).resample(period).agg({'timestamp':'size', 'user':'first', 'sourceIP':'first'}).fillna(0)    \n",
    "    \n",
    "    # Per computer, per process, find the count (sum), standard deviation, mean, and \n",
    "    # variance of the count of each process launch\n",
    "    subgroup = grouped.groupby(['computername', 'process']).agg({'timestamp':['sum','std','mean','var'], 'user':'first', 'sourceIP':'first'}).fillna(0)\n",
    "    \n",
    "    # Find the CV and RV of the counts of execution, per machine, per process\n",
    "    # Coefficient of variance (\"CV\") = sigma/mu = stdev/mean\n",
    "    # Relative variance (\"RV\") = sigma^2/mu = stdev^2/mean\n",
    "    subgroup['cv'] = subgroup['timestamp']['std']/subgroup['timestamp']['mean']\n",
    "    subgroup['rv'] = pow(subgroup['timestamp']['std'], 2)/subgroup['timestamp']['mean']\n",
    "    \n",
    "    # Include the beacon interval in the output\n",
    "    subgroup['beaconinterval'] = period\n",
    "    \n",
    "    # Now create a dataframe of ONLY rows that are in our target ranges as defined above.\n",
    "    # Find entries that fall in the indicated CV/RV ranges, which are most likely to be \"beacon-like\" processes.\n",
    "    # (Only include them if they executed a minimum number of times.)  Append to the list of \"found\" rows.\n",
    "    newlyfound = (subgroup[(subgroup['rv'] > (rvtarget - rvrange)) & (subgroup['rv'] < (rvtarget + rvrange)) & \n",
    "          (subgroup['cv'] > (cvtarget - cvrange)) & (subgroup['cv'] < (cvtarget + cvrange)) & \n",
    "          ((subgroup['timestamp']['sum']) > minimum_executions)]) \n",
    "    found = found.append(newlyfound)\n",
    "        \n",
    "    def get_first_timestamp(x):\n",
    "    # For each line of the \"Found\" dataframe, grab the computer name and process name from the row's index.\n",
    "    # Then perform a select operation on the original df, to find the first timestamp of execution.\n",
    "        \n",
    "        # These values come from the index (multiindex) of the passed-in row\n",
    "        compname = x.name[0]\n",
    "        procname = x.name[1]\n",
    "\n",
    "        # Return to the original dataframe to look up the first timestamp of execution, per machine per process\n",
    "        returnvalue = df[ (df['computername'] == compname) & (df['process'] == procname) ]['timestamp'][0]\n",
    "        return returnvalue\n",
    "    \n",
    "    # If we found anything, grab the index of the first timestamp where it occurred\n",
    "    if not found.empty:\n",
    "        found['firsttimestamp'] = found.apply(get_first_timestamp, axis=1)\n",
    "   \n",
    "if not found.empty:\n",
    "    found.reset_index(inplace=True) #flattens the dataframe \n",
    "    found.columns = ['computername', 'foundprocess', 'beaconinterval', 'coeffofvariance', 'firsttimestamp', \n",
    "                 'relativevariance', 'sourceIP', 'mean', 'std', 'executioncount', 'variance',\n",
    "                 'user']\n",
    "    found.to_csv(outfile, mode='a', index=False)\n",
    "    \n",
    "found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
